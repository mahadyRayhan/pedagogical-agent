{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pedagogical agents using LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create a pedagogical agent using LLM, we need three basic things:\n",
    "1. A vector database from knowledge base\n",
    "2. A LLM model\n",
    "3. A good prompt/question asking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector Database\n",
    "We directly use Gemini to process our data. Model can process upto 1 million tokens(~700k words or  50,000 lines of code with the standard 80 characters per line). This is roughly 2000 pages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os             # Provides functions to interact with the operating system\n",
    "import time           # Allows for time-related functions, including delays\n",
    "import glob           # Enables file pattern matching (e.g., finding all files with a certain extension)\n",
    "import google.generativeai as genai  # Imports Googleâ€™s generative AI library for accessing generative AI functions\n",
    "from dotenv import load_dotenv      # Handles environment variables by loading from a .env file\n",
    "from google.generativeai.types import HarmCategory, HarmBlockThreshold  # Allows for setting harm categories and block thresholds\n",
    "\n",
    "# Loading environment variables from the .env file\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GenAI Access and Initializing Core Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuring Google Generative AI with the API key from environment variables\n",
    "genai.configure(api_key=os.getenv(\"GOOGLE_API_KEY\"))\n",
    "\n",
    "# Initializing key variables for later use\n",
    "model = None               # Placeholder for the generative AI model instance\n",
    "safety_settings = None     # Placeholder for safety settings to manage response sensitivity\n",
    "files = None               # Placeholder for file handling, such as for file upload or processing\n",
    "chat_session = None        # Placeholder for managing a chat session with the AI model\n",
    "cached_responses = {}      # Dictionary to store previously generated responses for faster access\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Resource and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_to_gemini(paths, mime_type=None):\n",
    "    \"\"\"\n",
    "    Uploads specified files to the Gemini platform.\n",
    "\n",
    "    Parameters:\n",
    "    - paths (list): List of file paths to upload.\n",
    "    - mime_type (str, optional): The MIME type of the files (e.g., \"application/pdf\").\n",
    "                                 If None, the MIME type defaults to the file type.\n",
    "\n",
    "    Returns:\n",
    "    - files (list): A list of uploaded file objects, each containing metadata like display name and URI.\n",
    "\n",
    "    Workflow:\n",
    "    1. Iterates over each file path in `paths`.\n",
    "    2. Uploads the file to Gemini using `genai.upload_file()`.\n",
    "    3. Prints a confirmation message with the file's display name and URI after a successful upload.\n",
    "    4. Appends each uploaded file to the `files` list, which is returned at the end.\n",
    "    \"\"\"\n",
    "    \n",
    "    files = []\n",
    "    for path in paths:\n",
    "        # Upload each file to the generative AI platform, specifying MIME type if provided\n",
    "        file = genai.upload_file(path, mime_type=mime_type)\n",
    "        \n",
    "        # Print confirmation of each uploaded file\n",
    "        print(f\"Uploaded file '{file.display_name}' as: {file.uri}\")\n",
    "        \n",
    "        # Add the uploaded file object to the files list\n",
    "        files.append(file)\n",
    "    \n",
    "    return files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wait_for_files_active(files):\n",
    "    \"\"\"\n",
    "    Waits for each uploaded file to complete processing and become active on the Gemini platform.\n",
    "\n",
    "    Parameters:\n",
    "    - files (list): List of uploaded file objects to check for active status.\n",
    "\n",
    "    Workflow:\n",
    "    1. Displays a message to indicate waiting for file processing.\n",
    "    2. For each file in `files`, checks its processing state on the Gemini platform.\n",
    "    3. Repeatedly queries the file's state every 10 seconds until it becomes \"ACTIVE\".\n",
    "    4. Raises an exception if a file fails to reach the \"ACTIVE\" state, indicating a processing error.\n",
    "    5. Once all files are active, prints a success message.\n",
    "\n",
    "    Raises:\n",
    "    - Exception: If any file fails to process and reach the \"ACTIVE\" state.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Waiting for file processing...\")\n",
    "    for name in (file.name for file in files):\n",
    "        # Retrieve the current status of the file\n",
    "        file = genai.get_file(name)\n",
    "        \n",
    "        # Check the file status in a loop until it is marked as \"ACTIVE\"\n",
    "        while file.state.name == \"PROCESSING\":\n",
    "            print(\".\", end=\"\", flush=True)  # Indicate processing with dots\n",
    "            time.sleep(10)  # Wait 10 seconds before checking again\n",
    "            file = genai.get_file(name)\n",
    "        \n",
    "        # Raise an error if file processing failed\n",
    "        if file.state.name != \"ACTIVE\":\n",
    "            raise Exception(f\"File {file.name} failed to process\")\n",
    "    \n",
    "    # Success message when all files are ready for use\n",
    "    print(\"...all files ready\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def configure_genai():\n",
    "    \"\"\"\n",
    "    Configures the generative AI model with safety settings and generation parameters, and starts a chat session.\n",
    "\n",
    "    Workflow:\n",
    "    1. Defines safety settings to manage the filtering of specific content categories.\n",
    "    2. Sets generation configuration parameters for controlling response generation.\n",
    "    3. Instantiates the `GenerativeModel` with specified settings.\n",
    "    4. Initializes a chat session with the model for interactive conversations.\n",
    "\n",
    "    Attributes:\n",
    "    - safety_settings (dict): Configuration for blocking various content harm categories (e.g., hate speech, harassment).\n",
    "    - generation_config (dict): Settings that control generation behavior, such as `temperature` and `max_output_tokens`.\n",
    "    - model (GenerativeModel): The generative model instance configured with the specified parameters.\n",
    "    - chat_session (ChatSession): A chat session object for managing interactions with the model.\n",
    "    \"\"\"\n",
    "    global model, safety_settings, chat_session, generation_config\n",
    "    # Define safety settings for different types of harmful content\n",
    "    safety_settings = {\n",
    "        HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,\n",
    "        HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,\n",
    "        HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,\n",
    "        HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,\n",
    "    }\n",
    "    \n",
    "    # Configure generation parameters to control response characteristics\n",
    "    generation_config = {\n",
    "        \"temperature\": 0,              # Determines randomness in generation; 0 means deterministic output\n",
    "        \"top_p\": 0.95,                 # Controls cumulative probability for sampling\n",
    "        \"top_k\": 64,                   # Limits the sampling pool to the top 64 tokens\n",
    "        \"max_output_tokens\": 8192,     # Sets the maximum number of tokens for the generated response\n",
    "        \"response_mime_type\": \"text/plain\",  # Specifies response type as plain text\n",
    "    }\n",
    "    \n",
    "    # Initialize the generative AI model with specified settings\n",
    "    model = genai.GenerativeModel(model_name=\"gemini-1.5-flash\", generation_config=generation_config)\n",
    "    \n",
    "    # Start a new chat session for ongoing interactions\n",
    "    chat_session = model.start_chat()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_resources(load_resource=False):\n",
    "    \"\"\"\n",
    "    Loads resource files into the system, specifically uploading PDFs from the 'uSucceed_resource' directory\n",
    "    to the Gemini AI platform if necessary. \n",
    "\n",
    "    Parameters:\n",
    "    - load_resource (bool): If True, forces re-loading of resource files. Defaults to False.\n",
    "\n",
    "    Workflow:\n",
    "    1. Checks if resource files need to be loaded, based on `load_resource` or the existence of `files`.\n",
    "    2. Uses `glob` to find all PDFs in the specified 'uSucceed_resource' folder.\n",
    "    3. Calls `upload_to_gemini()` to upload the PDFs to Gemini for further processing.\n",
    "    4. Invokes `wait_for_files_active()` to ensure all files are uploaded and ready for use.\n",
    "    5. Configures the generative AI system with `configure_genai()` for effective processing.\n",
    "    \"\"\"\n",
    "    global files\n",
    "    if load_resource or not files:\n",
    "        # Retrieve all PDF files in the specified directory\n",
    "        file_paths = glob.glob(\"../uSucceed_resource/*.pdf\")\n",
    "        \n",
    "        # Upload files to the generative AI platform with specified MIME type\n",
    "        files = upload_to_gemini(file_paths, mime_type=\"application/pdf\")\n",
    "        \n",
    "        # Ensure the files are active and ready for use\n",
    "        wait_for_files_active(files)\n",
    "        \n",
    "        # Set up the generative AI with any necessary configurations\n",
    "        configure_genai()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model configuration\n",
    "For any LLM moels, we need a model configuration or model text generation configuration. This actually control models text generation capabilities. <br>\n",
    "Temperature: controls output generation. For T=0, The model produces deterministic output, meaning it will always generate the same response given the same input. This is for consistency. <br> <br>\n",
    "But for Gemini, we need one more configuration. which is security parameters. Google is very concerned about their security. So you need to configure the security parameters:\n",
    "- HATE_SPEECH\n",
    "- HARASSMENT\n",
    "- SEXUALLY_EXPLICIT\n",
    "- DANGEROUS_CONTENT\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query Type Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_query_type(query: str) -> str:\n",
    "    \"\"\"\n",
    "    Detects the type of query based on keywords, helping prioritize relevant documents.\n",
    "\n",
    "    Parameters:\n",
    "    - query (str): The user's input question or query.\n",
    "\n",
    "    Returns:\n",
    "    - str: A category label for the query type, either 'location', 'cybersecurity', 'system', or 'other'.\n",
    "\n",
    "    Workflow:\n",
    "    1. Defines sets of keywords for three categories: location-based, cybersecurity-related, and system-related queries.\n",
    "    2. Converts the query to lowercase for case-insensitive matching.\n",
    "    3. Checks if any keywords from each category are present in the query.\n",
    "    4. Returns the corresponding category if keywords are found, or 'other' if no keywords match.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define keyword lists for each query category\n",
    "    location_keywords = ['room', 'location', 'where', 'place', 'area', 'task']\n",
    "    security_keywords = ['security', 'cyber', 'attack', 'threat', 'protection']\n",
    "    system_keywords = ['controller', 'system', 'asset', 'configuration']\n",
    "    \n",
    "    # Convert query to lowercase for case-insensitive comparison\n",
    "    query_lower = query.lower()\n",
    "    \n",
    "    # Match keywords to categorize the query type\n",
    "    if any(keyword in query_lower for keyword in location_keywords):\n",
    "        return 'location'\n",
    "    elif any(keyword in query_lower for keyword in security_keywords):\n",
    "        return 'cybersecurity'\n",
    "    elif any(keyword in query_lower for keyword in system_keywords):\n",
    "        return 'system'\n",
    "    \n",
    "    # Default return for queries that don't match any category\n",
    "    return 'other'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query Processing and Answer Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompt(query_type: str, query: str) -> str:\n",
    "    \"\"\"Generate context-aware prompt based on query type\"\"\"\n",
    "    prompts = {\n",
    "        'location': \"\"\"You are a location-aware assistant. For questions about rooms or tasks:\n",
    "                     1. ONLY use information from 'Rooms_And_Tasks.pdf'\n",
    "                     2. Ignore all other documents completely for room/task questions\n",
    "                     3. Provide specific task details for the requested room\n",
    "                     4. If the information isn't in Rooms_And_Tasks.pdf, say \"I cannot find information about this room/task in the available documents.\"\n",
    "                     \n",
    "                     Question: {query}\"\"\",\n",
    "                     \n",
    "        'cybersecurity': \"\"\"You are a cybersecurity expert. For security questions:\n",
    "                           1. Prioritize information from cybersecurity guides and best practices\n",
    "                           2. Provide specific, actionable security information\n",
    "                           3. Only include room or system information if directly relevant to security\n",
    "                           \n",
    "                           Question: {query}\"\"\",\n",
    "        \n",
    "        'system': \"\"\"You are a system configuration assistant. For system questions:\n",
    "                    1. Focus on technical configuration and asset details\n",
    "                    2. Reference room information only if relevant to system setup\n",
    "                    3. Include security considerations only if directly applicable\n",
    "                    \n",
    "                    Question: {query}\"\"\",\n",
    "        \n",
    "        'other': \"\"\"You are a helpful assistant. Please answer the following question:\n",
    "                   {query}\"\"\"\n",
    "    }\n",
    "    \n",
    "    return prompts[query_type].format(query=query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query processing\n",
    "We want our system to be able to answer questions both context and content based.\n",
    "- Rule based classification\n",
    "- Generate final prompt based on the classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_answer(query: str):\n",
    "    \"\"\"\n",
    "    Processes a user query, generates an answer using loaded files, \n",
    "    and returns the answer along with execution times for various parts of the process.\n",
    "\n",
    "    Parameters:\n",
    "    - query (str): The user's question or query that needs to be answered.\n",
    "\n",
    "    Returns:\n",
    "    - tuple: A tuple containing the generated answer (str) and a dictionary of execution times (dict).\n",
    "\n",
    "    Workflow:\n",
    "    1. Checks if any files are loaded; raises an exception if none are available.\n",
    "    2. Records the time taken for initial checks.\n",
    "    3. Checks the cache for previously answered queries to optimize response time.\n",
    "    4. Detects the type of query (location, cybersecurity, system, etc.) to prioritize relevant documents.\n",
    "    5. Prepares a prompt for the generative model, including the user's question.\n",
    "    6. Selects and prioritizes relevant files based on the query type, adjusting the history context accordingly.\n",
    "    7. Sends the prepared prompt to the chat session and measures the response time.\n",
    "    8. Caches the generated response for future reference.\n",
    "    9. Returns the generated answer and timing metrics.\n",
    "    \n",
    "    Exception Handling:\n",
    "    - If an error occurs during processing, it logs the error message along with the time taken until the error occurred.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize timing dictionary and start overall timing\n",
    "    timings = {}\n",
    "    start_time = time.time()\n",
    "    global history\n",
    "\n",
    "    try:\n",
    "        # Ensure files are loaded for processing\n",
    "        if not files:\n",
    "            raise Exception(\"No files loaded. Please load files first using load_files()\")\n",
    "        \n",
    "        # Record time taken for initial file check\n",
    "        timings['initial_check'] = time.time() - start_time\n",
    "        \n",
    "        # Check cache first to save time on repeated queries\n",
    "        if query in cached_responses:\n",
    "            return cached_responses[query], {'cached': True, 'total_time': time.time() - start_time}\n",
    "\n",
    "        # Prepare the chat context and prompt for the query\n",
    "        query_type = detect_query_type(query)\n",
    "        # prompt_intro = \"You are a question answering model. Please answer the following question briefly and to the point. If there are multiple points found, give them all as it is and list them in a list. The question is:\"\n",
    "        # prompt = prompt_intro + \" \" + query\n",
    "        prompt = generate_prompt(query_type, query)\n",
    "        \n",
    "        # Prioritize relevant files for location queries\n",
    "        relevant_files_start_time = time.time()\n",
    "        relevant_files = files\n",
    "\n",
    "        # Separate and prioritize files based on query type\n",
    "        if query_type == 'location':\n",
    "            prioritized_files = [file for file in files if 'Rooms_And_Tasks.pdf' in file.display_name]\n",
    "            other_files = [file for file in files if 'Rooms_And_Tasks.pdf' not in file.display_name]\n",
    "            relevant_files = prioritized_files + other_files\n",
    "            \n",
    "        # Record time taken for relevant files selection\n",
    "        timings['relevant_files_selection'] = time.time() - relevant_files_start_time\n",
    "        \n",
    "        # Optimize the history update to keep full content only in prioritized files\n",
    "        history = []\n",
    "        for file in relevant_files:\n",
    "            if query_type == 'location' and 'Rooms_And_Tasks.pdf' in file.display_name:\n",
    "                history.append({\n",
    "                    \"role\": \"user\",\n",
    "                    \"parts\": [file]  # Full content for prioritized files\n",
    "                })\n",
    "            else:\n",
    "                # Use minimal info for other files\n",
    "                history.append({\n",
    "                    \"role\": \"user\",\n",
    "                    \"parts\": [f\"Basic content from {file.display_name}\"]\n",
    "                })\n",
    "        \n",
    "        # Add prompt to history\n",
    "        history.append({\n",
    "            \"role\": \"user\",\n",
    "            \"parts\": [prompt]\n",
    "        })\n",
    "        \n",
    "        # Apply history if this is a location query or complex question\n",
    "        if query_type == 'location':\n",
    "            chat_session.history = history\n",
    "        \n",
    "        # Get response from chat session\n",
    "        response_start_time = time.time()\n",
    "        response = chat_session.send_message(prompt, safety_settings=safety_settings)\n",
    "        \n",
    "        end_time = time.time()\n",
    "        timings['response_time'] = end_time - response_start_time  # Time for getting response\n",
    "        timings['total_time'] = end_time - start_time  # Total time taken\n",
    "        \n",
    "        # Cache the response for future queries\n",
    "        cached_responses[query] = response.text\n",
    "        \n",
    "        return response.text, timings  # Return the generated answer and timing breakdown\n",
    "        \n",
    "    except Exception as e:\n",
    "        end_time = time.time()\n",
    "        print(f\"\\nError occurred after {end_time - start_time:.2f} seconds\")\n",
    "        print(f\"Error processing query: {str(e)}\")\n",
    "        return f\"An error occurred: {str(e)}\", timings  # Return the error message and timings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cache and History (inference time optimization)\n",
    "- Cache:  re-fetch data for identical queries, leading to faster response times\n",
    "- History:\n",
    "    1. Contextual Awareness: Maintains conversation context for better responses.\n",
    "    2. Prioritization of Relevant Information: Provides full content for key files based on query_type.\n",
    "    3. Efficiency: Uses minimal info for less critical files, speeding up responses.\n",
    "    4. Improved Response Quality: Generates nuanced responses based on diverse user inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PA Question and Answering System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gemini_qa_system(query, load_resource=True):\n",
    "    \"\"\"\n",
    "    Manages the Gemini Question and Answering system, handling resource loading, \n",
    "    evaluation, and query processing based on the provided parameters.\n",
    "\n",
    "    Parameters:\n",
    "    - query (str): The user query for which an answer is requested. Defaults to an empty string.\n",
    "    - load_resource (bool): Indicates whether to load resources before processing. Defaults to True.\n",
    "\n",
    "    Workflow:\n",
    "    1. Initializes an instance of the GeminiQuestion_and_Answering class.\n",
    "    2. Loads resources if the load_resource parameter is set to True.\n",
    "    3. If a query is provided, retrieves and prints the generated answer from the system.\n",
    "\n",
    "    Returns:\n",
    "    - None: The function prints the generated answer to the console if a query is provided.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load resources if specified\n",
    "    if load_resource:\n",
    "        load_resources(load_resource=True)\n",
    "        \n",
    "    answer = get_answer(query)\n",
    "    import pprint\n",
    "    pprint.pprint(answer)\n",
    "    print(f\"Generated Answer: {answer}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test PA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded file '10 cybersecurity best practices.pdf' as: https://generativelanguage.googleapis.com/v1beta/files/wbsg0ku3agex\n",
      "Uploaded file '2024 Cybersecurity Guide by norton.pdf' as: https://generativelanguage.googleapis.com/v1beta/files/50ut9qypb8ld\n",
      "Uploaded file '6 types of cybersecurity attacks.pdf' as: https://generativelanguage.googleapis.com/v1beta/files/8phkyzsc9de\n",
      "Uploaded file 'CONTEXT_Rooms_And_Tasks.pdf' as: https://generativelanguage.googleapis.com/v1beta/files/9gjjeht1i172\n",
      "Uploaded file 'DDoS-Attacks.pdf' as: https://generativelanguage.googleapis.com/v1beta/files/pda0kbrsdmhw\n",
      "Uploaded file 'Direction for how to use controllers in uSucceed.docx.pdf' as: https://generativelanguage.googleapis.com/v1beta/files/1yo9zvkotowu\n",
      "Uploaded file 'Directory Design Idea.pdf' as: https://generativelanguage.googleapis.com/v1beta/files/9awo3q7pfww6\n",
      "Uploaded file 'Learning-based_Image_Analytics_in_User-AI_Agent_Interactions_for_Cyber-enabled_Manufacturing.pdf' as: https://generativelanguage.googleapis.com/v1beta/files/423bq99wuu8w\n",
      "Waiting for file processing...\n",
      "...all files ready\n",
      "\n",
      "('- Watch video\\n- Run System Health check \\n',\n",
      " {'initial_check': 0.0,\n",
      "  'relevant_files_selection': 0.0010030269622802734,\n",
      "  'response_time': 1.1106674671173096,\n",
      "  'total_time': 1.112668752670288})\n",
      "Generated Answer: ('- Watch video\\n- Run System Health check \\n', {'initial_check': 0.0, 'relevant_files_selection': 0.0010030269622802734, 'response_time': 1.1106674671173096, 'total_time': 1.112668752670288})\n"
     ]
    }
   ],
   "source": [
    "query=\"I am at 'Room 1'. what is my task here?\"\n",
    "gemini_qa_system(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
